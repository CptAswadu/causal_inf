{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cptas\\AppData\\Local\\Temp\\ipykernel_2948\\701211712.py:1: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('C:/Users/cptas/accepted_2007_to_2018Q4.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/cptas/accepted_2007_to_2018Q4.csv')\n",
    "n_df = df[['loan_amnt', 'term', 'int_rate', 'home_ownership',\n",
    "        'annual_inc', 'verification_status', 'purpose', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec',\n",
    "        'revol_bal', 'total_acc', 'initial_list_status',\n",
    "        'application_type', 'loan_status']]\n",
    "n_df = n_df.dropna()\n",
    "n_df['purpose'] = np.where((n_df.purpose == 'debt_consolidation') |\n",
    "                (n_df.purpose == 'credit_card') |\n",
    "                (n_df.purpose== \"house\") |\n",
    "                (n_df.purpose == 'home_improvement'), 1, 0)\n",
    "\n",
    "n_df['loan_status'] = np.where((n_df.loan_status == 'Current') |\n",
    "                (n_df.loan_status == 'Fully Paid') |\n",
    "                (n_df.loan_status== \"Issued\") |\n",
    "                (n_df.loan_status == 'Does not meet the credit policy. Status:Fully Paid'), 1, 0)\n",
    "\n",
    "#scale data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "n_df['loan_amnt'] = scaler.fit_transform(n_df[['loan_amnt']])\n",
    "n_df['int_rate'] = scaler.fit_transform(n_df[['int_rate']])\n",
    "n_df['annual_inc'] = scaler.fit_transform(n_df[['annual_inc']])\n",
    "n_df['delinq_2yrs'] = scaler.fit_transform(n_df[['delinq_2yrs']])\n",
    "n_df['inq_last_6mths'] = scaler.fit_transform(n_df[['inq_last_6mths']])\n",
    "n_df['open_acc'] = scaler.fit_transform(n_df[['open_acc']])\n",
    "n_df['pub_rec'] = scaler.fit_transform(n_df[['pub_rec']])\n",
    "n_df['revol_bal'] = scaler.fit_transform(n_df[['revol_bal']])\n",
    "n_df['total_acc'] = scaler.fit_transform(n_df[['total_acc']])\n",
    "\n",
    "# Select variables\n",
    "X = n_df[['loan_amnt', 'term', 'int_rate', 'home_ownership',\n",
    "        'annual_inc', 'verification_status', 'purpose', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec',\n",
    "        'revol_bal', 'total_acc', 'initial_list_status',\n",
    "        'application_type']]\n",
    "\n",
    "# Encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, columns=['term', 'home_ownership',\n",
    "                                       'verification_status',\n",
    "                                       'initial_list_status',\n",
    "                                       'application_type'])\n",
    "\n",
    "y = n_df['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\.conda\\lib\\site-packages\\torch\\nn\\functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(torch\u001b[39m.\u001b[39munique(sensitive))):\n\u001b[0;32m     85\u001b[0m     pos_rates\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mmean(pred_labels[sensitive \u001b[39m==\u001b[39m j]))\n\u001b[1;32m---> 86\u001b[0m fair_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msum(torch\u001b[39m.\u001b[39;49mabs(torch\u001b[39m.\u001b[39;49mstack(pos_rates) \u001b[39m-\u001b[39;49m torch\u001b[39m.\u001b[39;49mmean(pred_labels)))\n\u001b[0;32m     87\u001b[0m loss_total \u001b[39m=\u001b[39m loss_cls \u001b[39m-\u001b[39m (lambda_adv \u001b[39m*\u001b[39m loss_adv) \u001b[39m+\u001b[39m (lambda_fair \u001b[39m*\u001b[39m fair_loss)\n\u001b[0;32m     88\u001b[0m loss_total\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.functional.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        out = nn.functional.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels, _ in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "    \n",
    "criterion_cls = nn.BCELoss()\n",
    "criterion_adv = nn.BCELoss()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define sensitive attribute\n",
    "sensitive_attr = 'purpose'\n",
    "\n",
    "# Create train and test dataloaders\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train.values), torch.Tensor(y_train.values), torch.Tensor(X_train[sensitive_attr].values))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.Tensor(X_test.values), torch.Tensor(y_test.values), torch.Tensor(X_test[sensitive_attr].values))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define model, optimizer, and loss function\n",
    "classifier = Classifier(input_size=X_train.shape[1], hidden_size=64, output_size=1)\n",
    "discriminator = Discriminator(input_size=X_train.shape[1], hidden_size=64)\n",
    "optimizer_cls = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "optimizer_adv = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "# Set hyperparameters\n",
    "num_epochs = 10\n",
    "lambda_adv = 0.1\n",
    "lambda_fair = 1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels, sensitive) in enumerate(train_dataloader):\n",
    "        # Train discriminator\n",
    "        optimizer_adv.zero_grad()\n",
    "        pred_sensitive = discriminator(inputs)\n",
    "        loss_adv = criterion_adv(pred_sensitive, sensitive.unsqueeze(1))\n",
    "        loss_adv.backward()\n",
    "        optimizer_adv.step()\n",
    "\n",
    "        # Train classifier\n",
    "        optimizer_cls.zero_grad()\n",
    "        pred_labels = classifier(inputs)\n",
    "        loss_cls = criterion_cls(pred_labels, labels.unsqueeze(1))\n",
    "        pred_sensitive = discriminator(inputs)\n",
    "        loss_adv = criterion_adv(pred_sensitive, sensitive.unsqueeze(1))\n",
    "        pos_rates = []\n",
    "        for j in range(len(torch.unique(sensitive))):\n",
    "            pos_rates.append(torch.mean(pred_labels[sensitive == j]))\n",
    "        fair_loss = torch.sum(torch.abs(torch.stack(pos_rates) - torch.mean(pred_labels)))\n",
    "        loss_total = loss_cls - (lambda_adv * loss_adv) + (lambda_fair * fair_loss)\n",
    "        loss_total.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "# Evaluation\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for inputs, labels, sensitive in test_dataloader:\n",
    "        outputs = classifier(inputs)\n",
    "        predicted = (outputs >= 0.5).squeeze().long()\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(predicted.tolist())\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_scores = []\n",
    "y_true = []\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, sensitive in test_dataloader:\n",
    "        outputs = classifier(inputs)\n",
    "        y_scores.extend(outputs.tolist())\n",
    "        y_true.extend(labels.tolist())\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "print(\"ROC-AUC score:\", roc_auc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
